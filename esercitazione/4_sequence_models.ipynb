{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "RkaCypHnaBHX",
      "metadata": {
        "id": "RkaCypHnaBHX"
      },
      "source": [
        "# Character-Level Language Modeling with LSTM in PyTorch\n",
        "\n",
        "In this notebook, we will:\n",
        "- Load and preprocess Shakespeare's complete works (character-level).\n",
        "- Implement a simple LSTM-based model to predict the next character.\n",
        "- Train the model and generate new text by sampling from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TK7tPSrRaBHb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK7tPSrRaBHb",
        "outputId": "bd093489-da2c-4119-b179-99743da5b2d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: 72\n",
            "[' ', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’']\n"
          ]
        }
      ],
      "source": [
        "# Load Shakespeare from Project Gutenberg\n",
        "import requests\n",
        "import re\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# Extract main content\n",
        "start_marker = \"THE SONNETS\"\n",
        "end_marker = \"End of the Project Gutenberg EBook\"\n",
        "start_idx = text.find(start_marker)\n",
        "end_idx = text.find(end_marker)\n",
        "text = text[start_idx:end_idx]\n",
        "\n",
        "# Normalize whitespace\n",
        "text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "# Optionally truncate for faster training\n",
        "text = text[:20000]\n",
        "\n",
        "# Build vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Unique characters: {vocab_size}\\n{chars}\")\n",
        "\n",
        "# Mappings\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
        "\n",
        "# Encode entire text\n",
        "encoded = [char_to_idx[c] for c in text]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vltcorfvaBHd",
      "metadata": {
        "id": "vltcorfvaBHd"
      },
      "source": [
        "## Create input/output sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CLj6pb0daBHd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLj6pb0daBHd",
        "outputId": "b72de03e-21da-4af4-e75d-9244489aa8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([19900, 100]) torch.Size([19900, 100])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "seq_length = 100\n",
        "step = 1\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for i in range(0, len(encoded) - seq_length, step):\n",
        "    inputs.append(encoded[i:i + seq_length])\n",
        "    targets.append(encoded[i + 1:i + seq_length + 1])\n",
        "\n",
        "# Convert to tensors\n",
        "X = torch.tensor(inputs, dtype=torch.long)\n",
        "y = torch.tensor(targets, dtype=torch.long)\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SAs1xRTIaBHd",
      "metadata": {
        "id": "SAs1xRTIaBHd"
      },
      "source": [
        "## Define the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A-KUIXawaBHe",
      "metadata": {
        "id": "A-KUIXawaBHe"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QUE-1Zk3aBHe",
      "metadata": {
        "id": "QUE-1Zk3aBHe"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0KZfYkOWaBHf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KZfYkOWaBHf",
        "outputId": "cea3f97d-59a0-4956-bc75-4b23e4d5302c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 1.4149\n",
            "Epoch 2/10, Loss: 0.3483\n",
            "Epoch 3/10, Loss: 0.1639\n",
            "Epoch 4/10, Loss: 0.1367\n",
            "Epoch 5/10, Loss: 0.1253\n",
            "Epoch 6/10, Loss: 0.1188\n",
            "Epoch 7/10, Loss: 0.1143\n",
            "Epoch 8/10, Loss: 0.1106\n",
            "Epoch 9/10, Loss: 0.1080\n",
            "Epoch 10/10, Loss: 0.1060\n"
          ]
        }
      ],
      "source": [
        "model = CharRNN(vocab_size=vocab_size, hidden_size=256)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create dataset and DataLoader for batching\n",
        "batch_size = 64\n",
        "dataset = TensorDataset(X, y)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(xb)\n",
        "        loss = criterion(output.view(-1, vocab_size), yb.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IGEEEDS-aBHf",
      "metadata": {
        "id": "IGEEEDS-aBHf"
      },
      "source": [
        "## Generate text from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-CEJ1gcPaBHf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CEJ1gcPaBHf",
        "outputId": "8449e4ff-29a0-481f-b45d-d596a72d8754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To be noth lire in this pence on the trese high dead, By boare window, And windows distress wilt abudly die. but not waid but not wail doth sweets wanted looks nature’s etcreding sight: For then in my thie thought beauthe or eyed, Doth rient ten fore-bed, A wan I for recoment of trusbed ear, Mine wellate widow well wayth shame And baren were it wees your life? For beauty o’er in hide on the resposed with adonous sweets want all her fave thine with beauteous rose desired not live betual falst prove no\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate(model, start_seq, length):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[c] for c in start_seq], dtype=torch.long).unsqueeze(0)\n",
        "    generated = start_seq\n",
        "    hidden = None\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            last_logits = output[0, -1]\n",
        "            p = torch.softmax(last_logits, dim=0).cpu().numpy()\n",
        "            next_idx = np.random.choice(len(p), p=p)\n",
        "            next_char = idx_to_char[next_idx]\n",
        "            generated += next_char\n",
        "            input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[next_idx]])], dim=1)\n",
        "    return generated\n",
        "\n",
        "print(generate(model, start_seq=\"To be\", length=500))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3VBaJQbBaBHg",
      "metadata": {
        "id": "3VBaJQbBaBHg"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. Train on the full corpus.\n",
        "2. Improve the generation quality:\n",
        "   - Add temperature sampling.\n",
        "   - Use a deeper or bidirectional LSTM.\n",
        "   - Add dropout or layer normalization.\n",
        "3. Explore variants:\n",
        "   - Replace LSTM with GRU.\n",
        "   - Add attention."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
