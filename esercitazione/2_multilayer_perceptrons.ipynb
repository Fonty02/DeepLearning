{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIjd5xH87QiW"
      },
      "source": [
        "# Introduction to Multilayer Perceptrons with PyTorch\n",
        "\n",
        "In this notebook, we will:\n",
        "1. Load and preprocess a digit classification dataset.\n",
        "2. Get a quick introduction to PyTorch basics.\n",
        "3. Implement a Multilayer Perceptron (MLP) for digit classification in PyTorch."
      ],
      "id": "BIjd5xH87QiW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piu5a09L7Qia"
      },
      "source": [
        "## Loading and Preprocessing the Digits Dataset"
      ],
      "id": "piu5a09L7Qia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk-2hl1j7Qib",
        "outputId": "fc71dc79-c3b6-4938-9cb4-ce24287a97d0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Train shape:\", X_train_scaled.shape)\n",
        "print(\"Test shape:\", X_test_scaled.shape)"
      ],
      "id": "Lk-2hl1j7Qib",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (1437, 64)\n",
            "Test shape: (360, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "cmpnyxut7Qic",
        "outputId": "44fb9ea4-7d0d-4add-f73e-fb3227a632fc"
      },
      "source": [
        "def plot_digits(X, y, indices):\n",
        "    n = len(indices)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i, idx in enumerate(indices):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        img = X[idx].reshape(8, 8)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f'Label: {y[idx]}')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "plot_digits(X, y, range(5))"
      ],
      "id": "cmpnyxut7Qic",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsOxcuei7Qic"
      },
      "source": [
        "## Quick Introduction to PyTorch\n",
        "\n",
        "PyTorch is a popular deep learning framework that allows for:\n",
        "- Flexible tensor computations (similar to NumPy but with GPU acceleration).\n",
        "- Automatic differentiation via `autograd`.\n",
        "- Easy model definition using `nn.Module`.\n",
        "\n",
        "Here’s a quick demo of PyTorch basics."
      ],
      "id": "NsOxcuei7Qic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDeWe1_r7Qic",
        "outputId": "17feba06-4342-43e1-f107-9b613fe4dda2"
      },
      "source": [
        "import torch\n",
        "\n",
        "# AUTOGRAD EXAMPLE: derivative of f(x) = x^2 + 2x + 1\n",
        "x = torch.tensor([3.0], requires_grad=True)\n",
        "y = x**2 + 2*x + 1\n",
        "y.backward()\n",
        "print(\"df/dx at x=3 (autograd):\", x.grad.item())\n",
        "\n",
        "# BASIC TENSOR SHAPES AND OPERATIONS\n",
        "t = torch.randn(8, 2, 4)\n",
        "print(\"shape:\", t.shape)\n",
        "\n",
        "# view / reshape\n",
        "t2 = t.reshape(8, -1)\n",
        "print(\"reshape:\", t2.shape)\n",
        "\n",
        "# permute (reorder dimensions)\n",
        "t3 = t.permute(0, 2, 1)\n",
        "print(\"permute:\", t3.shape)\n",
        "\n",
        "# unsqueeze / squeeze\n",
        "v = torch.randn(10)\n",
        "v_u = v.unsqueeze(0)\n",
        "print(\"unsqueeze:\", v_u.shape)\n",
        "\n",
        "# repeat (replication)\n",
        "a = torch.tensor([[1., 2.]])\n",
        "a_rep = a.repeat(3, 1)\n",
        "print(\"repeat:\\n\", a_rep)\n",
        "\n",
        "# matrix multiplication (@ is equivalent to torch.matmul)\n",
        "A = torch.randn(5, 3)\n",
        "B = torch.randn(3, 4)\n",
        "C = A @ B\n",
        "print(\"matmul shape:\", C.shape)\n",
        "\n",
        "# scatter: simple one-hot encoding\n",
        "idx = torch.tensor([2, 0, 1])\n",
        "one_hot = torch.zeros(3, 3)\n",
        "one_hot.scatter_(1, idx.unsqueeze(1), 1.)\n",
        "print(\"one-hot:\\n\", one_hot)"
      ],
      "id": "bDeWe1_r7Qic",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df/dx at x=3 (autograd): 8.0\n",
            "shape: torch.Size([8, 2, 4])\n",
            "reshape: torch.Size([8, 8])\n",
            "permute: torch.Size([8, 4, 2])\n",
            "unsqueeze: torch.Size([1, 10])\n",
            "repeat:\n",
            " tensor([[1., 2.],\n",
            "        [1., 2.],\n",
            "        [1., 2.]])\n",
            "matmul shape: torch.Size([5, 4])\n",
            "one-hot:\n",
            " tensor([[0., 0., 1.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 1., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V0Lnpnn7Qid",
        "outputId": "f39e0fcc-5fc0-40a0-8da1-92dd86a488a4"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert training data to torch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Check a batch\n",
        "for batch_X, batch_y in train_loader:\n",
        "    print(batch_X.shape, batch_y.shape)\n",
        "    break"
      ],
      "id": "8V0Lnpnn7Qid",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 64]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST8nDxle7Qid"
      },
      "source": [
        "## Building an MLP in PyTorch (Guided Example)\n",
        "\n",
        "We will create a simple MLP with:\n",
        "- Input layer: 64 features (8x8 image flattened)\n",
        "- Hidden layer: 128 neurons, ReLU activation\n",
        "- Output layer: 10 classes (digits 0–9)"
      ],
      "id": "ST8nDxle7Qid"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z4O5ZDo7Qid",
        "outputId": "811aa2b4-335e-4280-a8f9-16301293cdab"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DigitClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(64, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # raw logits\n",
        "        return x\n",
        "\n",
        "# Instantiate model, loss, optimizer\n",
        "model = DigitClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
      ],
      "id": "_z4O5ZDo7Qid",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.0547\n",
            "Epoch 2, Loss: 0.4560\n",
            "Epoch 3, Loss: 0.2937\n",
            "Epoch 4, Loss: 0.3917\n",
            "Epoch 5, Loss: 0.1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0NBnwZ07Qie",
        "outputId": "c366af84-7f17-4bbf-a961-d703d696355d"
      },
      "source": [
        "# Convert test data\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)\n",
        "\n",
        "# Evaluate accuracy\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    acc = (predicted == y_test_tensor).float().mean().item()\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")"
      ],
      "id": "q0NBnwZ07Qie",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v-bVS5g7Qie"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. MLP from scratch\n",
        "   - Reimplement the above PyTorch MLP without looking at the provided code.\n",
        "   - Aim for 97% accuracy.\n",
        "\n",
        "2. Bonus\n",
        "   - Apply the same MLP architecture to the MNIST dataset (28x28 images).\n",
        "   - Compare results with Digits."
      ],
      "id": "4v-bVS5g7Qie"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}