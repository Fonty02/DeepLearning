{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3df8267",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 3\n",
    "## Emanuele Fontana\n",
    "\n",
    "In this notebook we'll try to reach 90% accuracy on CIFAR-10 with 2 approaches:\n",
    "1. Using a simple Convolutional Neural Network (CNN) built with PyTorch\n",
    "2. Using Transfer Learning with a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb14174",
   "metadata": {},
   "source": [
    "### Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031c853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define transformations for CNN training (32x32)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define transformations for ResNet18 training (224x224 with ImageNet normalization)\n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),  # Ridimensiona a 224x224 per ResNet18\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # ImageNet normalization (mean)\n",
    "                         (0.229, 0.224, 0.225))   # ImageNet normalization (std)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Custom dataset with multiple augmented versions\n",
    "class AugmentedCIFAR10(torchvision.datasets.CIFAR10):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_augmentations = 3\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) * self.num_augmentations\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original image and label\n",
    "        original_idx = idx // self.num_augmentations\n",
    "        image_array = self.data[original_idx]\n",
    "        label = self.targets[original_idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image\n",
    "        image = Image.fromarray(image_array)\n",
    "        \n",
    "        if idx % self.num_augmentations == 0:\n",
    "            #original image\n",
    "            newImage = image\n",
    "        elif idx % self.num_augmentations == 1:\n",
    "            #Vertical Flip\n",
    "            newImage = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif idx % self.num_augmentations == 2:\n",
    "            # Horizontal Flip\n",
    "            newImage = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        \n",
    "        newImage = self.transform(newImage)\n",
    "        return newImage, label\n",
    "\n",
    "# Load dataset with augmentation for CNN\n",
    "trainset = AugmentedCIFAR10(root='./data', train=True,\n",
    "                            download=True, transform=transform) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1024,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# Load dataset with augmentation for ResNet18\n",
    "trainset_resnet = AugmentedCIFAR10(root='./data', train=True,\n",
    "                                   download=True, transform=transform_resnet) \n",
    "trainloader_resnet = torch.utils.data.DataLoader(trainset_resnet, batch_size=1024,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "testset_resnet = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                              download=True, transform=transform_resnet)\n",
    "testloader_resnet = torch.utils.data.DataLoader(testset_resnet, batch_size=1024,\n",
    "                                                shuffle=False)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b5331",
   "metadata": {},
   "source": [
    "### 1 - Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Block 1: 3 -> 8 feature maps\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)  \n",
    "        self.dropout1 = nn.Dropout2d(p=0.5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 8 -> 16 feature maps\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)  \n",
    "        self.dropout2 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        # Block 3: 16 -> 32 feature maps\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32) \n",
    "        self.dropout3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        # MLP head for classification\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 256)\n",
    "        self.bc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.pool(self.dropout1(self.bn1(F.relu(self.conv1(x)))))\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.pool(self.dropout2(self.bn2(F.relu(self.conv2(x)))))\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = self.pool(self.dropout3(self.bn3(F.relu(self.conv3(x)))))\n",
    "        \n",
    "        # Flatten for MLP\n",
    "        x = x.view(-1, 32 * 4 * 4)\n",
    "        \n",
    "        # MLP\n",
    "        x = F.relu(self.bc1(self.fc1(x)))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022d03b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Reset train metrics each epoch\u001b[39;00m\n\u001b[32m     24\u001b[39m metrics[\u001b[33m\"\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m\"\u001b[39m].reset()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.ConstantLR(optimizer, factor=0.5, total_iters=5)\n",
    "# Define torchmetrics objects\n",
    "metrics = {\n",
    "    \"train_acc\": Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "    \"test_acc\": Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "}\n",
    "\n",
    "patience = 5\n",
    "not_improved_epochs = 0\n",
    "best_test_acc = 0.0\n",
    "\n",
    "# Training\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Reset train metrics each epoch\n",
    "    metrics[\"train_acc\"].reset()\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update training metrics\n",
    "        metrics[\"train_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute train metrics\n",
    "    train_acc = metrics[\"train_acc\"].compute().item()\n",
    "\n",
    "    # Reset test metrics\n",
    "    metrics[\"test_acc\"].reset()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            metrics[\"test_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_acc = metrics[\"test_acc\"].compute().item()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        not_improved_epochs = 0\n",
    "    else:\n",
    "        not_improved_epochs += 1\n",
    "    \n",
    "    if not_improved_epochs >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler based on test accuracy\n",
    "    scheduler.step(test_acc)\n",
    "\n",
    "    # Log results\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {running_loss/len(trainloader):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67659ed",
   "metadata": {},
   "source": [
    "### 2 - Transfer Learning with Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f58d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "# Load pre-trained ResNet18\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "# Replace the final classification layer for 10 classes\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Define optimizer and loss function for ResNet\n",
    "criterion_resnet = nn.CrossEntropyLoss()\n",
    "optimizer_resnet = optim.AdamW(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Start fine-tuning ResNet model by learning only the classification head\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "epochs_resnet = 30\n",
    "patience_resnet = 5\n",
    "not_improved_epochs_resnet = 0\n",
    "best_test_acc_resnet = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(epochs_resnet):\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Reset train metrics each epoch\n",
    "    metrics[\"train_acc\"].reset()\n",
    "\n",
    "    for images, labels in trainloader_resnet:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_resnet.zero_grad()\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion_resnet(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_resnet.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update training metrics\n",
    "        metrics[\"train_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute train metrics\n",
    "    train_acc = metrics[\"train_acc\"].compute().item()\n",
    "\n",
    "    # Reset test metrics\n",
    "    metrics[\"test_acc\"].reset()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader_resnet:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = resnet(images)\n",
    "            metrics[\"test_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_acc = metrics[\"test_acc\"].compute().item()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc_resnet:\n",
    "        best_test_acc_resnet = test_acc\n",
    "        not_improved_epochs_resnet = 0\n",
    "    else:\n",
    "        not_improved_epochs_resnet += 1\n",
    "    \n",
    "    if not_improved_epochs_resnet >= patience_resnet:\n",
    "        print(\"Early stopping triggered for ResNet18\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler based on test accuracy\n",
    "    scheduler.step(test_acc)\n",
    "\n",
    "    # Log results\n",
    "    print(f\"ResNet18 Epoch [{epoch+1}/{epochs_resnet}] | Loss: {running_loss/len(trainloader_resnet):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
