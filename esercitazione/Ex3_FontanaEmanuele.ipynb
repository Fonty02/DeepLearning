{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3df8267",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 3\n",
    "## Emanuele Fontana\n",
    "\n",
    "In this notebook we'll try to reach 90% accuracy on CIFAR-10 with 2 approaches:\n",
    "1. Using a simple Convolutional Neural Network (CNN) built with PyTorch\n",
    "2. Using Transfer Learning with a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb14174",
   "metadata": {},
   "source": [
    "### Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define transformations for CNN training (32x32)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define transformations for ResNet18 training (224x224 with ImageNet normalization)\n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),  # Ridimensiona a 224x224 per ResNet18\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # ImageNet normalization (mean)\n",
    "                         (0.229, 0.224, 0.225))   # ImageNet normalization (std)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Custom dataset with multiple augmented versions\n",
    "class AugmentedCIFAR10(torchvision.datasets.CIFAR10):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_augmentations = 3\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) * self.num_augmentations\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original image and label\n",
    "        original_idx = idx // self.num_augmentations\n",
    "        image_array = self.data[original_idx]\n",
    "        label = self.targets[original_idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image\n",
    "        image = Image.fromarray(image_array)\n",
    "        \n",
    "        if idx % self.num_augmentations == 0:\n",
    "            #original image\n",
    "            newImage = image\n",
    "        elif idx % self.num_augmentations == 1:\n",
    "            #Vertical Flip\n",
    "            newImage = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif idx % self.num_augmentations == 2:\n",
    "            # Horizontal Flip\n",
    "            newImage = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        \n",
    "        newImage = self.transform(newImage)\n",
    "        return newImage, label\n",
    "\n",
    "# Load dataset with augmentation for CNN\n",
    "trainset = AugmentedCIFAR10(root='./data', train=True,\n",
    "                            download=True, transform=transform) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1024,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024,\n",
    "                                         shuffle=False)\n",
    "\n",
    "# Load dataset with augmentation for ResNet18\n",
    "trainset_resnet = AugmentedCIFAR10(root='./data', train=True,\n",
    "                                   download=True, transform=transform_resnet) \n",
    "trainloader_resnet = torch.utils.data.DataLoader(trainset_resnet, batch_size=1024,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "testset_resnet = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                              download=True, transform=transform_resnet)\n",
    "testloader_resnet = torch.utils.data.DataLoader(testset_resnet, batch_size=1024,\n",
    "                                                shuffle=False)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b5331",
   "metadata": {},
   "source": [
    "### 1 - Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Block 1: 3 -> 8 feature maps\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)  # Deve corrispondere ai canali di conv1\n",
    "        self.dropout1 = nn.Dropout2d(p=0.5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 8 -> 16 feature maps\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)  # Deve corrispondere ai canali di conv2\n",
    "        self.dropout2 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        # Block 3: 16 -> 32 feature maps\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)  # Deve corrispondere ai canali di conv3\n",
    "        self.dropout3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        # MLP head for classification\n",
    "        # Dopo 3 pooling: 32x32 -> 16x16 -> 8x8 -> 4x4\n",
    "        # Feature maps finali: 32 (non 128)\n",
    "        # Dimensione input MLP: 32 * 4 * 4 = 512\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 256)\n",
    "        self.bc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.pool(self.dropout1(self.bn1(F.relu(self.conv1(x)))))\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.pool(self.dropout2(self.bn2(F.relu(self.conv2(x)))))\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = self.pool(self.dropout3(self.bn3(F.relu(self.conv3(x)))))\n",
    "        \n",
    "        # Flatten for MLP\n",
    "        x = x.view(-1, 32 * 4 * 4)\n",
    "        \n",
    "        # MLP\n",
    "        x = F.relu(self.bc1(self.fc1(x)))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d03b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 8 elements not 32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m images, labels = images.to(device), labels.to(device)\n\u001b[32m     28\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     31\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# First convolutional block\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool(\u001b[38;5;28mself\u001b[39m.dropout1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Second convolutional block\u001b[39;00m\n\u001b[32m     32\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool(\u001b[38;5;28mself\u001b[39m.dropout2(\u001b[38;5;28mself\u001b[39m.bn2(F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x)))))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: running_mean should contain 8 elements not 32"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.ConstantLR(optimizer, factor=0.5, total_iters=5)\n",
    "# Define torchmetrics objects\n",
    "metrics = {\n",
    "    \"train_acc\": Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "    \"test_acc\": Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "}\n",
    "\n",
    "patience = 5\n",
    "not_improved_epochs = 0\n",
    "best_test_acc = 0.0\n",
    "\n",
    "# Training\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Reset train metrics each epoch\n",
    "    metrics[\"train_acc\"].reset()\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update training metrics\n",
    "        metrics[\"train_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute train metrics\n",
    "    train_acc = metrics[\"train_acc\"].compute().item()\n",
    "\n",
    "    # Reset test metrics\n",
    "    metrics[\"test_acc\"].reset()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            metrics[\"test_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_acc = metrics[\"test_acc\"].compute().item()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        not_improved_epochs = 0\n",
    "    else:\n",
    "        not_improved_epochs += 1\n",
    "    \n",
    "    if not_improved_epochs >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler based on test accuracy\n",
    "    scheduler.step(test_acc)\n",
    "\n",
    "    # Log results\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {running_loss/len(trainloader):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67659ed",
   "metadata": {},
   "source": [
    "### 2 - Transfer Learning with Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f58d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to C:\\Users\\fonta/.cache\\torch\\hub\\checkpoints\\vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "# Load pre-trained ResNet18\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "# Replace the final classification layer for 10 classes\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# Define optimizer and loss function for ResNet\n",
    "criterion_resnet = nn.CrossEntropyLoss()\n",
    "optimizer_resnet = optim.AdamW(resnet.parameters(), lr=0.001)\n",
    "\n",
    "# Start fine-tuning ResNet model by learning only the classification head\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in resnet.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "epochs_resnet = 30\n",
    "patience_resnet = 5\n",
    "not_improved_epochs_resnet = 0\n",
    "best_test_acc_resnet = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(epochs_resnet):\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Reset train metrics each epoch\n",
    "    metrics[\"train_acc\"].reset()\n",
    "\n",
    "    for images, labels in trainloader_resnet:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_resnet.zero_grad()\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion_resnet(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_resnet.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update training metrics\n",
    "        metrics[\"train_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute train metrics\n",
    "    train_acc = metrics[\"train_acc\"].compute().item()\n",
    "\n",
    "    # Reset test metrics\n",
    "    metrics[\"test_acc\"].reset()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader_resnet:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = resnet(images)\n",
    "            metrics[\"test_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_acc = metrics[\"test_acc\"].compute().item()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc_resnet:\n",
    "        best_test_acc_resnet = test_acc\n",
    "        not_improved_epochs_resnet = 0\n",
    "    else:\n",
    "        not_improved_epochs_resnet += 1\n",
    "    \n",
    "    if not_improved_epochs_resnet >= patience_resnet:\n",
    "        print(\"Early stopping triggered for ResNet18\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler based on test accuracy\n",
    "    scheduler.step(test_acc)\n",
    "\n",
    "    # Log results\n",
    "    print(f\"ResNet18 Epoch [{epoch+1}/{epochs_resnet}] | Loss: {running_loss/len(trainloader_resnet):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
