{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3df8267",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 3\n",
    "## Emanuele Fontana\n",
    "\n",
    "In this notebook we'll try to reach 90% accuracy on CIFAR-10 with 2 approaches:\n",
    "1. Using a simple Convolutional Neural Network (CNN) built with PyTorch\n",
    "2. Using Transfer Learning with a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb14174",
   "metadata": {},
   "source": [
    "### Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "031c853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define transformations for CNN training (32x32)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define transformations for effnet18 training (224x224 with ImageNet normalization)\n",
    "transform_effnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ridimensiona a 224x224 per effnet18\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # ImageNet normalization (mean)\n",
    "                         (0.229, 0.224, 0.225))   # ImageNet normalization (std)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Custom dataset with multiple augmented versions\n",
    "class AugmentedCIFAR10(torchvision.datasets.CIFAR10):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_augmentations = 3\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) * self.num_augmentations\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original image and label\n",
    "        original_idx = idx // self.num_augmentations\n",
    "        image_array = self.data[original_idx]\n",
    "        label = self.targets[original_idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image\n",
    "        image = Image.fromarray(image_array)\n",
    "        \n",
    "        if idx % self.num_augmentations == 0:\n",
    "            #original image\n",
    "            newImage = image\n",
    "        elif idx % self.num_augmentations == 1:\n",
    "            #Vertical Flip\n",
    "            newImage = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif idx % self.num_augmentations == 2:\n",
    "            # Horizontal Flip\n",
    "            newImage = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        \n",
    "        newImage = self.transform(newImage)\n",
    "        return newImage, label\n",
    "\n",
    "# Load dataset with augmentation for CNN\n",
    "trainset = AugmentedCIFAR10(root='./data', train=True,\n",
    "                            download=True, transform=transform) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=512,\n",
    "                                         shuffle=False)\n",
    "\n",
    "\n",
    "# Load dataset with augmentation for effnet18\n",
    "trainset_effnet = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                   download=True, transform=transform_effnet) \n",
    "trainloader_effnet = torch.utils.data.DataLoader(trainset_effnet, batch_size=64,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "testset_effnet = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                              download=True, transform=transform_effnet)\n",
    "testloader_effnet = torch.utils.data.DataLoader(testset_effnet, batch_size=64,\n",
    "                                                shuffle=False)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b5331",
   "metadata": {},
   "source": [
    "### 1 - Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da39f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.rel1=nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(16)  \n",
    "        self.dropout1 = nn.Dropout2d(p=0.5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.leakyrelu=nn.LeakyReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(32)  \n",
    "        self.dropout2 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.gelu=nn.GELU()\n",
    "        self.bn3 = nn.BatchNorm2d(64) \n",
    "        self.dropout3 = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "        # MLP head for classification\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.bc1 = nn.BatchNorm1d(128)\n",
    "        self.dropout_fc = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.pool(self.dropout1(self.bn1(self.rel1(self.conv1(x)))))\n",
    "        # Second convolutional block\n",
    "        x = self.pool(self.dropout2(self.bn2(self.leakyrelu(self.conv2(x)))))\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = self.pool(self.dropout3(self.bn3(self.gelu(self.conv3(x)))))\n",
    "        \n",
    "        # Flatten for MLP\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        \n",
    "        # MLP\n",
    "        x = F.relu(self.bc1(self.fc1(x)))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "022d03b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] | Loss: 1.8906 | Train Acc: 0.3024 | Test Acc: 0.4389\n",
      "Epoch [2/30] | Loss: 1.6753 | Train Acc: 0.3833 | Test Acc: 0.4959\n",
      "Epoch [3/30] | Loss: 1.6105 | Train Acc: 0.4084 | Test Acc: 0.5263\n",
      "Epoch [4/30] | Loss: 1.5640 | Train Acc: 0.4269 | Test Acc: 0.5530\n",
      "Epoch [5/30] | Loss: 1.5358 | Train Acc: 0.4405 | Test Acc: 0.5732\n",
      "Epoch [6/30] | Loss: 2.0136 | Train Acc: 0.2270 | Test Acc: 0.3014\n",
      "Epoch [7/30] | Loss: 1.9141 | Train Acc: 0.2629 | Test Acc: 0.3476\n",
      "Epoch [8/30] | Loss: 1.8739 | Train Acc: 0.2801 | Test Acc: 0.3795\n",
      "Epoch [9/30] | Loss: 1.8514 | Train Acc: 0.2903 | Test Acc: 0.3737\n",
      "Epoch [10/30] | Loss: 1.8399 | Train Acc: 0.2947 | Test Acc: 0.4183\n",
      "Epoch [11/30] | Loss: 1.8320 | Train Acc: 0.2988 | Test Acc: 0.4242\n",
      "Epoch [12/30] | Loss: 1.8147 | Train Acc: 0.3069 | Test Acc: 0.4523\n",
      "Epoch [13/30] | Loss: 1.8105 | Train Acc: 0.3111 | Test Acc: 0.4492\n",
      "Epoch [14/30] | Loss: 1.8111 | Train Acc: 0.3098 | Test Acc: 0.4265\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.1)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.ConstantLR(optimizer, factor=0.01, total_iters=5)\n",
    "# Define torchmetrics objects\n",
    "metrics = {\n",
    "    \"train_acc\": Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "    \"test_acc\": Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "}\n",
    "\n",
    "patience = 10\n",
    "not_improved_epochs = 0\n",
    "best_test_acc = 0.0\n",
    "\n",
    "# Training\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Reset train metrics each epoch\n",
    "    metrics[\"train_acc\"].reset()\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update training metrics\n",
    "        metrics[\"train_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute train metrics\n",
    "    train_acc = metrics[\"train_acc\"].compute().item()\n",
    "\n",
    "    # Reset test metrics\n",
    "    metrics[\"test_acc\"].reset()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            metrics[\"test_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_acc = metrics[\"test_acc\"].compute().item()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        not_improved_epochs = 0\n",
    "    else:\n",
    "        not_improved_epochs += 1\n",
    "    \n",
    "    if not_improved_epochs >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Log results\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {running_loss/len(trainloader):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67659ed",
   "metadata": {},
   "source": [
    "### 2 - Transfer Learning with Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f58d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet-B0 Epoch [1/30] | Loss: 5.0685 | Train Acc: 0.6516 | Test Acc: 0.7293\n",
      "EfficientNet-B0 Epoch [2/30] | Loss: 5.5770 | Train Acc: 0.6681 | Test Acc: 0.7264\n",
      "EfficientNet-B0 Epoch [3/30] | Loss: 5.6775 | Train Acc: 0.6715 | Test Acc: 0.7319\n",
      "EfficientNet-B0 Epoch [4/30] | Loss: 5.7203 | Train Acc: 0.6673 | Test Acc: 0.6858\n",
      "EfficientNet-B0 Epoch [5/30] | Loss: 5.8351 | Train Acc: 0.6681 | Test Acc: 0.7274\n",
      "EfficientNet-B0 Epoch [6/30] | Loss: 5.8288 | Train Acc: 0.6692 | Test Acc: 0.7212\n",
      "EfficientNet-B0 Epoch [7/30] | Loss: 5.7136 | Train Acc: 0.6698 | Test Acc: 0.7237\n",
      "EfficientNet-B0 Epoch [8/30] | Loss: 5.6361 | Train Acc: 0.6708 | Test Acc: 0.7397\n",
      "EfficientNet-B0 Epoch [9/30] | Loss: 5.7923 | Train Acc: 0.6696 | Test Acc: 0.7342\n",
      "EfficientNet-B0 Epoch [10/30] | Loss: 5.8284 | Train Acc: 0.6684 | Test Acc: 0.7405\n",
      "EfficientNet-B0 Epoch [11/30] | Loss: 5.7387 | Train Acc: 0.6698 | Test Acc: 0.7169\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "# Load pre-trained EfficientNet-B0\n",
    "effnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "# Replace the final classification layer for 10 classes\n",
    "effnet.classifier[1] = nn.Linear(effnet.classifier[1].in_features, num_classes)\n",
    "effnet = effnet.to(device)\n",
    "\n",
    "# Define optimizer and loss function for effnet\n",
    "criterion_effnet = nn.CrossEntropyLoss()\n",
    "optimizer_effnet = optim.AdamW(effnet.parameters(), lr=0.1)\n",
    "\n",
    "# Start fine-tuning effnet model by learning only the classification head\n",
    "# Freeze all backbone parameters\n",
    "for param in effnet.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze only the classifier head\n",
    "for param in effnet.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "epochs_effnet = 30\n",
    "patience_effnet = 5\n",
    "not_improved_epochs_effnet = 0\n",
    "best_test_acc_effnet = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(epochs_effnet):\n",
    "    effnet.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Reset train metrics each epoch\n",
    "    metrics[\"train_acc\"].reset()\n",
    "\n",
    "    for images, labels in trainloader_effnet:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_effnet.zero_grad()\n",
    "        outputs = effnet(images)\n",
    "        loss = criterion_effnet(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_effnet.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update training metrics\n",
    "        metrics[\"train_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute train metrics\n",
    "    train_acc = metrics[\"train_acc\"].compute().item()\n",
    "\n",
    "    # Reset test metrics\n",
    "    metrics[\"test_acc\"].reset()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    effnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader_effnet:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = effnet(images)\n",
    "            metrics[\"test_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_acc = metrics[\"test_acc\"].compute().item()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc_effnet:\n",
    "        best_test_acc_effnet = test_acc\n",
    "        not_improved_epochs_effnet = 0\n",
    "    else:\n",
    "        not_improved_epochs_effnet += 1\n",
    "    \n",
    "    if not_improved_epochs_effnet >= patience_effnet:\n",
    "        print(\"Early stopping triggered for EfficientNet-B0\")\n",
    "        break\n",
    "\n",
    "    # Log results\n",
    "    print(f\"EfficientNet-B0 Epoch [{epoch+1}/{epochs_effnet}] | Loss: {running_loss/len(trainloader_effnet):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
