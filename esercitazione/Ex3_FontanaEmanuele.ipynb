{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3df8267",
   "metadata": {},
   "source": [
    "# Deep Learning - Exercise 3\n",
    "## Emanuele Fontana\n",
    "\n",
    "In this notebook we'll try to reach 90% accuracy on CIFAR-10 with 2 approaches:\n",
    "1. Using a simple Convolutional Neural Network (CNN) built with PyTorch\n",
    "2. Using Transfer Learning with a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb14174",
   "metadata": {},
   "source": [
    "### Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031c853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define transformations for CNN training (32x32)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define transformations for effnet18 training (224x224 with ImageNet normalization)\n",
    "transform_effnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ridimensiona a 224x224 per effnet18\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # ImageNet normalization (mean)\n",
    "                         (0.229, 0.224, 0.225))   # ImageNet normalization (std)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Custom dataset with multiple augmented versions\n",
    "class AugmentedCIFAR10(torchvision.datasets.CIFAR10):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_augmentations = 4\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) * self.num_augmentations\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the original image and label\n",
    "        original_idx = idx // self.num_augmentations\n",
    "        image_array = self.data[original_idx]\n",
    "        label = self.targets[original_idx]\n",
    "        \n",
    "        # Convert numpy array to PIL Image\n",
    "        image = Image.fromarray(image_array)\n",
    "        \n",
    "        if idx % self.num_augmentations == 0:\n",
    "            #original image\n",
    "            newImage = image\n",
    "        elif idx % self.num_augmentations == 1:\n",
    "            #Vertical Flip\n",
    "            newImage = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        elif idx % self.num_augmentations == 2:\n",
    "            # Horizontal Flip\n",
    "            newImage = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        else:\n",
    "            # Add some noise by multiplying with a random noise image\n",
    "            newImage= image.point(lambda p: p * torch.randn(1).item())\n",
    "\n",
    "        \n",
    "        newImage = self.transform(newImage)\n",
    "        return newImage, label\n",
    "\n",
    "# Load dataset with augmentation for CNN\n",
    "trainset = AugmentedCIFAR10(root='./data', train=True,\n",
    "                            download=True, transform=transform) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=512,\n",
    "                                         shuffle=False)\n",
    "\n",
    "\n",
    "# Load dataset with augmentation for effnet18\n",
    "trainset_effnet = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                   download=True, transform=transform_effnet) \n",
    "trainloader_effnet = torch.utils.data.DataLoader(trainset_effnet, batch_size=64,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "testset_effnet = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                              download=True, transform=transform_effnet)\n",
    "testloader_effnet = torch.utils.data.DataLoader(testset_effnet, batch_size=64,\n",
    "                                                shuffle=False)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b5331",
   "metadata": {},
   "source": [
    "### 1 - Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.gel1=nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm2d(64)  \n",
    "        self.dropout1 = nn.Dropout2d(p=0.25)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.gel2=nn.GELU()\n",
    "        self.bn2 = nn.BatchNorm2d(128)  \n",
    "        self.dropout2 = nn.Dropout2d(p=0.25)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.gel3=nn.GELU()\n",
    "        self.bn3 = nn.BatchNorm2d(256) \n",
    "        self.dropout3 = nn.Dropout2d(p=0.25)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        #Block 4\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.gel4=nn.GELU()\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.dropout4 = nn.Dropout2d(p=0.25)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # MLP head for classification\n",
    "        #First linear block\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 256)\n",
    "        self.fcre1lu=nn.ReLU()\n",
    "        self.bc1 = nn.BatchNorm1d(256)\n",
    "        #second linear block\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fcre2lu=nn.ReLU()\n",
    "        self.bc2 = nn.BatchNorm1d(128)\n",
    "        #output layer\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x=self.pool1(self.dropout1(self.bn1(self.gel1(self.conv1(x)))))\n",
    "        # Second convolutional block\n",
    "        x=self.pool2(self.dropout2(self.bn2(self.gel2(self.conv2(x)))))\n",
    "        # Third convolutional block\n",
    "        x=self.pool3(self.dropout3(self.bn3(self.gel3(self.conv3(x)))))\n",
    "        # Fourth convolutional block\n",
    "        x=self.pool4(self.dropout4(self.bn4(self.gel4(self.conv4(x)))))\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Fully connected layer 1\n",
    "        x=self.bc1(self.fcre1lu(self.fc1(x)))\n",
    "        # Fully connected layer 2\n",
    "        x=self.bc2(self.fcre2lu(self.fc2(x)))\n",
    "        # Output layer\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "022d03b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (2 x 2). Kernel size: (3 x 3). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m images, labels = images.to(device), labels.to(device)\n\u001b[32m     28\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     31\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     48\u001b[39m x=\u001b[38;5;28mself\u001b[39m.pool2(\u001b[38;5;28mself\u001b[39m.dropout2(\u001b[38;5;28mself\u001b[39m.bn2(\u001b[38;5;28mself\u001b[39m.gel2(\u001b[38;5;28mself\u001b[39m.conv2(x)))))\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Third convolutional block\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m x=\u001b[38;5;28mself\u001b[39m.pool3(\u001b[38;5;28mself\u001b[39m.dropout3(\u001b[38;5;28mself\u001b[39m.bn3(\u001b[38;5;28mself\u001b[39m.gel3(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))))\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Fourth convolutional block\u001b[39;00m\n\u001b[32m     52\u001b[39m x=\u001b[38;5;28mself\u001b[39m.pool4(\u001b[38;5;28mself\u001b[39m.dropout4(\u001b[38;5;28mself\u001b[39m.bn4(\u001b[38;5;28mself\u001b[39m.gel4(\u001b[38;5;28mself\u001b[39m.conv4(x)))))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fonta\\Desktop\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Calculated padded input size per channel: (2 x 2). Kernel size: (3 x 3). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.1)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "# Define torchmetrics objects\n",
    "metrics = {\n",
    "    \"train_acc\": Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "    \"test_acc\": Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "}\n",
    "\n",
    "patience = 10\n",
    "not_improved_epochs = 0\n",
    "best_test_acc = 0.0\n",
    "\n",
    "# Training\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Reset train metrics each epoch\n",
    "    metrics[\"train_acc\"].reset()\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update training metrics\n",
    "        metrics[\"train_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute train metrics\n",
    "    train_acc = metrics[\"train_acc\"].compute().item()\n",
    "\n",
    "    # Reset test metrics\n",
    "    metrics[\"test_acc\"].reset()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            metrics[\"test_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_acc = metrics[\"test_acc\"].compute().item()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        not_improved_epochs = 0\n",
    "    else:\n",
    "        not_improved_epochs += 1\n",
    "    \n",
    "    if not_improved_epochs >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Log results\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {running_loss/len(trainloader):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67659ed",
   "metadata": {},
   "source": [
    "### 2 - Transfer Learning with Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f58d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\fonta\\Desktop\\Uni\\Repo\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 Epoch [1/30] | Loss: 1.1966 | Train Acc: 0.6147 | Test Acc: 0.7576\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = 10\n",
    "# Load pre-trained EfficientNet-B0\n",
    "effnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
    "\n",
    "# Replace the final classification layer for 10 classes\n",
    "effnet.classifier[1] = nn.Linear(effnet.classifier[1].in_features, num_classes)\n",
    "effnet = effnet.to(device)\n",
    "\n",
    "# Define optimizer and loss function for effnet\n",
    "criterion_effnet = nn.CrossEntropyLoss()\n",
    "optimizer_effnet = optim.AdamW(effnet.parameters(), lr=0.1)\n",
    "\n",
    "# Start fine-tuning effnet model by learning only the classification head\n",
    "# Freeze all backbone parameters\n",
    "for param in effnet.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze only the classifier head\n",
    "for param in effnet.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "epochs_effnet = 30\n",
    "patience_effnet = 5\n",
    "not_improved_epochs_effnet = 0\n",
    "best_test_acc_effnet = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(epochs_effnet):\n",
    "    effnet.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Reset train metrics each epoch\n",
    "    metrics[\"train_acc\"].reset()\n",
    "\n",
    "    for images, labels in trainloader_effnet:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_effnet.zero_grad()\n",
    "        outputs = effnet(images)\n",
    "        loss = criterion_effnet(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_effnet.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update training metrics\n",
    "        metrics[\"train_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute train metrics\n",
    "    train_acc = metrics[\"train_acc\"].compute().item()\n",
    "\n",
    "    # Reset test metrics\n",
    "    metrics[\"test_acc\"].reset()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    effnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader_effnet:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = effnet(images)\n",
    "            metrics[\"test_acc\"].update(outputs, labels)\n",
    "\n",
    "    # Compute test metrics\n",
    "    test_acc = metrics[\"test_acc\"].compute().item()\n",
    "\n",
    "    # Early stopping logic\n",
    "    if test_acc > best_test_acc_effnet:\n",
    "        best_test_acc_effnet = test_acc\n",
    "        not_improved_epochs_effnet = 0\n",
    "    else:\n",
    "        not_improved_epochs_effnet += 1\n",
    "    \n",
    "    if not_improved_epochs_effnet >= patience_effnet:\n",
    "        print(\"Early stopping triggered for EfficientNet-B0\")\n",
    "        break\n",
    "\n",
    "    # Log results\n",
    "    print(f\"EfficientNet-B0 Epoch [{epoch+1}/{epochs_effnet}] | Loss: {running_loss/len(trainloader_effnet):.4f} | \"\n",
    "            f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
